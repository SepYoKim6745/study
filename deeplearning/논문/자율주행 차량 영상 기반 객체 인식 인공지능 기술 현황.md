
# 자율주행 차량 영상 기반 객체 인식 인공지능 기술 현황

> ※ 본 문서는 학습을 위한 문서로 내용이 부정확할 수 있습니다.

- 이번 논문을 읽는 목적은 자율주행 차량에서 영상 기반 객체 인식 인공지능 기술의 현황을 파악하고자이다.

</br>

## 기본 정보
- **저자 임헌국**
- **출판 연도(year)** : 2021
- **저널/컨퍼런스** : Journal of the Korea Institue of Infomation nad Communication Engineering
- **논문 링크** : http://jkiice.org/

</br>

## 1. 논문 요약
- **목표**: 영상 기반 객체 알고리즘 YOLO/SSD 알고리즘과 R-CNN/Faster R-CNN 알고리즘에 대해 분석한다.
- **핵심 기여**: 영상 기반 객체 검출 알고리즘 두 가지를 비교하여 상황에 따라 자율 주행 기술에 필요한 알고리즘을 선택하고 연구 개발하기를 기대한다.
- **결과**
    - 단일 단계 검출 알고리즘(YOLO/SSD 알고리즘)의 경우, 높은 속도를 가지고 낮은 정확도를 가진다는 특징이 있다.
    - 두 단계 검출 알고리즘(R-CNN/Faster R-CNN)의 경우, 낮은 속도를 가지며, 높은 정확도를 가진다는 특징이 있다.
    - 두 알고리즘의 특징을 이용해 선별적으로 이용할 수 있다.

## 2. 주요 개념 및 방법론
- **문제 정의**
    - 자율주행 차량 카메라는 정지된 영상이 아닌 연속적인 영상에서 객체를 인식해야하므로 객체를 인식하는데 제한이 있다.
    - 위 내용처럼 객체를 인식하기 위해서는 카메라를 이용한 영상 인식 기술에서 중요한 것은 속도와 정확도이다.
    - 아무리 정확한 알고리즘이라고 하더라도 속도가 느리면 안되고 빠른 알고리즘이라고 하더라고 정확한 알고리즘이 아니라면 자율주행에서 객체를 인식하는데 필요성이 없게 된다.
- **제안된 방법**
    - 객체 검출을 위해서는 객체의 **위치**를 찾고 객체가 무엇인지 인지하여 **분류**해야한다.
    - 그러기 위해서는 알고리즘이 두가지로 좁혀지게 되는데 단일 검출 방법과 두단계 검출 방법으로 나눠진다.
    - **단일 검출 방법** : 모든 영역에서 위치 검출과 분류를 동시에 수행하게 된다. => (높은 속도, 낮은 정확도)
        - **YOLO 알고리즘**
            - 합성곱 신경망을 통해 여러 바운딩 박스에 대한 클래스 확률을 계산하는 방식이다.
            - YOLO는 각 바운딩 박스를 예측하기 위해서 전체 이미지를 특징을 활용한다.
            - 이러한 특징으로 빠른 속도로 객체 검출이 가능하며 정확성을 유지할 수 있다.
            
            - 모델 구조 
                - 합성곱 계층 -> 전결합 계층으로 구성
                - 합성곱 계층 : 이미지로부터 특징을 추출
                - 전결합 계층 : 클래스 확률과 바운딩 박스의 좌표 예측

            - 단계
                1. YOLO 이미지 입력
                2. SxS 크기의 그리드로 나누기
                3. 각각의 그리드 셀은 B개의 네모 박스와 각 박스에 대한 신뢰 점수를 갖게 된다.</br>
                $Confidence Score(Cs) = pr(obj)*IOU$<sup>truthpred</sup>

        - **SSD 알고리즘**
            - 모델 구조
                - YOLO의 한계를 극복하기 위해 탄생한 기술이다.
                - YOLO로와는 다르게 컨볼루션 과정을 거치는 중간 중간 특징맵에서 모든 객체 검출을 수행한다.
                - (이유) 중간 중간 특징 맵에서 모든 객체를 검출하는 이유는 YOLO의 단점 때문인데 YOLO는 하나의 객체를 여러 그리드 셀이 동시에 검출할 가능성이 있기 때문에 SSD에서는 특징 맵에서 모든 객체를 검출한다.

            - 단계
                1. 합성곱 과정에서 중간에 특징 맵의 모든 객체 검출을 수행한다.
                2. 합성곱 과정을 계속 진행하여 최종적으로 1x1 크기의 특징 맵까지 추출한다.
                3. 추출된 특징 맵은 Detector & Classifier를 통과 시켜 객체 인식을 수행한다.
                
                </br>
    - **두 단계 검출 방법** : 객체 위치 검출과 분류가 순차적으로 이루어짐 => (낮은 속도, 높은 정확도)
        - **CNN 알고리즘**
            - 모델 구조
                - 합성곱 계층 -> 풀링 계층 -> 전연결 계층으로 구성
                - 합성곱 계층 : 패딩, 스트라이드를 통해 필터 연산을 수행하여 데이터의 크기를 줄여준다. 결과적으로 특징 맵을 추출한다.
                - 풀링 계층 : 가로, 세로 방향의 공간을 줄이는 연산 작업이다. (리소스, 시간 단축)
                - 전결합 계층 : 추출한 특징 맵을 분류하는 역할을 한다.

            - 단계
                1. 합성곱 계층에서 이미지에 필터를 적용하여 특징 맵 추출 (필터 적용 과정에서 패딩, 스트라이드 적용)
                2. 추출된 특징 맵의 주요 정보를 요약하기 위해 최대 풀링 기법을 활용해 맵을 축소한다.
                3. 풀링 계층을 거친 특징 맵을 전연결계층에서는 1차원 벡터로 변환하여 합성곱 계층으로 추출한 특징을 분류한다.
  
        - **R-CNN/Faster**
            - 
        
        - **R-CNN알고리즘**: (핵심 알고리즘과 그 단계)
  
## 3. 실험 및 결과
- **데이터셋**: (사용된 데이터셋과 데이터의 특징)
- **평가 방법**: (어떤 평가 지표가 사용되었는가?)
- **결과 분석**: (실험 결과와 성과에 대한 분석)

## 4. 장점과 한계
- **장점**: (논문에서 제안된 방법의 장점)
- **한계**: (논문의 한계점이나 개선할 부분)

## 5. 관련 연구 및 비교
- **관련 연구**: (이 논문이 다른 연구와 어떻게 비교되는가?)
- **차별점**: (논문의 독창적인 점과 기존 연구와의 차이)

## 6. 구현 및 코드
- **코드 링크**: [GitHub Repository](URL)
- **주요 구현 사항**: (코드에서 주목할 점과 핵심 구현 부분)

## 7. 개인적인 인사이트 및 응용 가능성
- **배운 점**: (이 논문에서 배운 점)
- **응용 가능성**: (자신의 프로젝트에 어떻게 적용할 수 있을지)
- **추가 연구 아이디어**: (이 논문을 기반으로 추가 연구할 아이디어)

## 8. 질문 및 추가 학습 필요 사항
- **이해가 안 되는 부분**: 
- **추가로 공부해야 할 개념**: 

## 9. 참고 문헌
- **관련 논문**: (참고할 만한 관련 논문 목록)

---

**마지막 수정**: YYYY-MM-DD


